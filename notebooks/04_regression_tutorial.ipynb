{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 04 — Predicting Glomerular Position from Odor Responses\n",
    "\n",
    "Glomeruli in the olfactory bulb are not randomly scattered — their\n",
    "positions reflect the functional organization of olfactory processing.\n",
    "In this notebook we ask: **can we predict *where* a glomerulus sits on\n",
    "the bulb from *how* it responds to odorants?**\n",
    "\n",
    "We will use this question to walk through the core ideas of regression\n",
    "in scikit-learn, building up in three stages:\n",
    "\n",
    "| Stage | Features | Targets | Goal |\n",
    "|-------|----------|---------|------|\n",
    "| 1 | 1 odorant | y position | Simple linear regression |\n",
    "| 2 | 2 odorants | y position | Multiple regression |\n",
    "| 3 | All odorants | x *and* y position | Multi-output regression |\n",
    "\n",
    "**Prerequisites:** Basic Python and NumPy. No prior machine-learning\n",
    "experience is assumed."
   ]
  },
  {
   "cell_type": "code",
   "id": "71i5h329s9",
   "source": "%pip install -q git+https://github.com/CastroLab/glom-explorer.git",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup and data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glom_explorer as gx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-explain",
   "metadata": {},
   "source": "### Loading the data\n\nThe dataset contains calcium-imaging responses from glomeruli on the\ndorsal surface of the mouse olfactory bulb.  Each glomerulus was\nstimulated with a panel of **59 odorants** at low concentration, and\nits fluorescence response was recorded.  We also know the **spatial\ncoordinates** (x, y) of every glomerulus.\n\n`load_clustered_variant()` returns two DataFrames (low- and\nhigh-concentration).  We will work with the **low-concentration**\ndata.  We use the `nonresponders_dropped` variant, which excludes\nglomeruli whose mean response across all odorants is near zero —\nthese contribute no usable signal and would add noise to our\nregressions.\n\nColumns are glomeruli and rows are odorants, so we transpose\nto get the conventional *samples × features* layout."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": "# Load odorant metadata and clustered response matrices\nodorants = gx.load_odorants()\nlow, high = gx.load_clustered_variant('nonresponders_dropped')\n\n# Odorant names (we will use these as feature labels)\nodorant_names = odorants['Name'].values\nprint(f\"Number of odorants: {len(odorant_names)}\")\nprint(f\"Number of glomeruli: {low.shape[1]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the feature matrix X (glomeruli × odorants)\n",
    "# and target vectors for y-position and x-position.\n",
    "\n",
    "X = low.values.T                          # shape: (n_glomeruli, 59)\n",
    "y_pos = low.columns.get_level_values('y').astype(float).values\n",
    "x_pos = low.columns.get_level_values('x').astype(float).values\n",
    "\n",
    "print(f\"Feature matrix X:  {X.shape}  (glomeruli × odorants)\")\n",
    "print(f\"Target y_pos:      {y_pos.shape}\")\n",
    "print(f\"Target x_pos:      {x_pos.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "### Quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 3.5))\n",
    "\n",
    "axes[0].hist(y_pos, bins=30, edgecolor='white')\n",
    "axes[0].set(xlabel='y position', ylabel='Count', title='Distribution of y positions')\n",
    "\n",
    "axes[1].hist(X.mean(axis=1), bins=30, edgecolor='white')\n",
    "axes[1].set(xlabel='Mean response', ylabel='Count',\n",
    "            title='Mean response per glomerulus')\n",
    "\n",
    "sc = axes[2].scatter(x_pos, y_pos, c=X.mean(axis=1), s=8, cmap='viridis', alpha=0.7)\n",
    "axes[2].set(xlabel='x position', ylabel='y position',\n",
    "            title='Glomeruli colored by mean response')\n",
    "plt.colorbar(sc, ax=axes[2], label='Mean ΔF/F')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Single feature, single target\n",
    "\n",
    "The simplest regression question we can ask: **can a glomerulus's\n",
    "response to one odorant predict its y position?**\n",
    "\n",
    "This is *simple linear regression* — fitting a line:\n",
    "\n",
    "$$\\hat{y} = w \\cdot x + b$$\n",
    "\n",
    "where $x$ is the response to a single odorant, $w$ is the slope\n",
    "(coefficient), and $b$ is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pick-odorant",
   "metadata": {},
   "source": "### 1.1 Choose an odorant and filter to responders\n\nLet's start with **butyric acid**, a carboxylic acid with a strong,\nrancid-butter smell.  It is odorant index 1 in our panel.\n\nEven after removing global non-responders, most glomeruli do not\nrespond to any *particular* odorant — olfactory receptors are\nselective.  If we regress on all glomeruli, the scatter plot will be\ndominated by a blob of zeros at every y position, drowning out the\nreal signal.  So for single-odorant regression we **filter to\nglomeruli that actually respond** (response > 0) to the chosen\nodorant."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-single",
   "metadata": {},
   "outputs": [],
   "source": "odorant_idx = 1\nprint(f\"Odorant: {odorant_names[odorant_idx]}\")\n\n# Filter to glomeruli that respond to this odorant\nresp_mask = X[:, odorant_idx] > 0\nX1_all = X[resp_mask]\ny1_pos = y_pos[resp_mask]\nfeature = X1_all[:, odorant_idx]\n\nprint(f\"Responders: {resp_mask.sum()} / {len(resp_mask)} glomeruli\")\n\nplt.figure(figsize=(6, 4))\nplt.scatter(feature, y1_pos, s=8, alpha=0.5)\nplt.xlabel(f'Response to {odorant_names[odorant_idx]}')\nplt.ylabel('y position')\nplt.title('Can one odorant predict position?')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "train-test-explain",
   "metadata": {},
   "source": [
    "### 1.2 Train / test split\n",
    "\n",
    "Before we fit a model, we need to **hold out** some data for\n",
    "evaluation.  If we tested on the same data we trained on, we would not\n",
    "know whether the model has truly learned a generalizable pattern or\n",
    "just memorized the training examples.\n",
    "\n",
    "`train_test_split` randomly partitions the data.  We use 80 % for\n",
    "training and reserve 20 % for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split-1",
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\n\n# Reshape to a column vector — sklearn expects 2-D input\nX1 = feature.reshape(-1, 1)\n\nX1_train, X1_test, y_train, y_test = train_test_split(\n    X1, y1_pos, test_size=0.2, random_state=42\n)\n\nprint(f\"Training samples: {X1_train.shape[0]}\")\nprint(f\"Test samples:     {X1_test.shape[0]}\")"
  },
  {
   "cell_type": "markdown",
   "id": "fit-explain",
   "metadata": {},
   "source": [
    "### 1.3 Fit a linear regression\n",
    "\n",
    "Scikit-learn follows a consistent API:\n",
    "\n",
    "1. **Instantiate** the model.\n",
    "2. **Fit** it on training data (`model.fit(X_train, y_train)`).\n",
    "3. **Predict** on new data (`model.predict(X_test)`).\n",
    "4. **Evaluate** with a scoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X1_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X1_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"Coefficient (slope): {model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept:           {model.intercept_:.4f}\")\n",
    "print(f\"R² (test):           {r2:.4f}\")\n",
    "print(f\"RMSE (test):         {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r2-explainer",
   "metadata": {},
   "source": [
    "> **What is R²?**  \n",
    "> R² (coefficient of determination) measures the fraction of variance\n",
    "> in the target that the model explains.  \n",
    "> - R² = 1 → perfect prediction  \n",
    "> - R² = 0 → no better than predicting the mean  \n",
    "> - R² < 0 → worse than predicting the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-fit-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Left: regression line over data\n",
    "x_line = np.linspace(X1_test.min(), X1_test.max(), 100).reshape(-1, 1)\n",
    "axes[0].scatter(X1_test, y_test, s=10, alpha=0.4, label='Test data')\n",
    "axes[0].plot(x_line, model.predict(x_line), 'r-', linewidth=2, label='Fit')\n",
    "axes[0].set(xlabel=f'Response to {odorant_names[odorant_idx]}',\n",
    "            ylabel='y position', title=f'Simple linear regression (R²={r2:.3f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: predicted vs actual\n",
    "axes[1].scatter(y_test, y_pred, s=10, alpha=0.4)\n",
    "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
    "axes[1].plot(lims, lims, 'k--', linewidth=1, label='Perfect prediction')\n",
    "axes[1].set(xlabel='Actual y position', ylabel='Predicted y position',\n",
    "            title='Predicted vs. Actual')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "survey-odorants",
   "metadata": {},
   "source": "### 1.4 Does the choice of odorant matter?\n\nSome odorants may carry more spatial information than others.  Let's\nfit a simple regression for *every* odorant (filtering to its\nresponders each time) and rank them by R²."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "odorant-survey",
   "metadata": {},
   "outputs": [],
   "source": "r2_per_odorant = []\nn_responders = []\n\nfor i in range(X.shape[1]):\n    mask = X[:, i] > 0\n    if mask.sum() < 10:  # skip odorants with too few responders\n        r2_per_odorant.append(np.nan)\n        n_responders.append(mask.sum())\n        continue\n    Xi = X[mask, i].reshape(-1, 1)\n    yi = y_pos[mask]\n    Xi_train, Xi_test, yi_train, yi_test = train_test_split(\n        Xi, yi, test_size=0.2, random_state=42\n    )\n    m = LinearRegression().fit(Xi_train, yi_train)\n    r2_per_odorant.append(r2_score(yi_test, m.predict(Xi_test)))\n    n_responders.append(mask.sum())\n\nr2_series = pd.Series(r2_per_odorant, index=odorant_names)\nr2_sorted = r2_series.dropna().sort_values(ascending=False)\n\nprint(\"Top 10 odorants by R² (single-feature, responders only):\")\nfor name in r2_sorted.head(10).index:\n    idx = list(odorant_names).index(name)\n    print(f\"  {name:40s}  R²={r2_sorted[name]:.4f}  (n={n_responders[idx]})\")\nprint(f\"\\nWorst R²: {r2_sorted.iloc[-1]:.4f} ({r2_sorted.index[-1]})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-odorant-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(r2_sorted)), r2_sorted.values, color='steelblue')\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.xticks(range(len(r2_sorted)), r2_sorted.index, rotation=90, fontsize=7)\n",
    "plt.ylabel('R² (test set)')\n",
    "plt.title('Predictive power of each odorant for y position')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage1-reflection",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "A single odorant captures only a sliver of the spatial information.\n",
    "Some odorants are slightly predictive, others are not.  This makes\n",
    "biological sense — each odorant activates a limited set of receptors\n",
    "and, by extension, a limited set of glomeruli.\n",
    "\n",
    "**Can we do better by combining information from multiple odorants?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Two features, single target\n",
    "\n",
    "We now use responses to **two odorants** as features:\n",
    "\n",
    "$$\\hat{y} = w_1 x_1 + w_2 x_2 + b$$\n",
    "\n",
    "This is *multiple* (or multivariate) linear regression — still one\n",
    "target, but more than one feature.  The model fits a **plane** through\n",
    "the data instead of a line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pick-two",
   "metadata": {},
   "source": "### 2.1 Choose two odorants\n\nWe keep **butyric acid** from Section 1 and search for the odorant\nthat best *complements* it — the one that most improves R² when\npaired with butyric acid.  The best partner is not necessarily the\nbest solo predictor; what matters is that the second feature captures\nspatial information that the first one misses."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select-two",
   "metadata": {},
   "outputs": [],
   "source": "# Rank all odorants by R² on all glomeruli (no responder filtering)\nr2_all_glom = []\nfor i in range(X.shape[1]):\n    Xi = X[:, i].reshape(-1, 1)\n    Xi_tr, Xi_te, yi_tr, yi_te = train_test_split(\n        Xi, y_pos, test_size=0.2, random_state=42\n    )\n    m = LinearRegression().fit(Xi_tr, yi_tr)\n    r2_all_glom.append(r2_score(yi_te, m.predict(Xi_te)))\n\nr2_all_sorted = pd.Series(r2_all_glom, index=odorant_names).sort_values(ascending=False)\n\n# Keep butyric acid (best single predictor on all glomeruli)\nbest_idx = odorant_idx  # from Section 1\nbest_name = odorant_names[best_idx]\n\n# Find the odorant that best complements it (highest joint R²)\nbest_r2_pair = -np.inf\nsecond_idx = None\nfor i in range(X.shape[1]):\n    if i == best_idx:\n        continue\n    X_pair = X[:, [best_idx, i]]\n    Xp_tr, Xp_te, yp_tr, yp_te = train_test_split(\n        X_pair, y_pos, test_size=0.2, random_state=42\n    )\n    m = LinearRegression().fit(Xp_tr, yp_tr)\n    r2_pair = r2_score(yp_te, m.predict(Xp_te))\n    if r2_pair > best_r2_pair:\n        best_r2_pair = r2_pair\n        second_idx = i\n\nsecond_name = odorant_names[second_idx]\n\nprint(f\"Feature 1: {odorants.loc[best_idx, 'Odorant']}  (solo R²={r2_all_glom[best_idx]:.4f})\")\nprint(f\"Feature 2: {odorants.loc[second_idx, 'Odorant']}  (solo R²={r2_all_glom[second_idx]:.4f})\")\nprint(f\"Pair R²:   {best_r2_pair:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-two",
   "metadata": {},
   "outputs": [],
   "source": "X2 = X[:, [best_idx, second_idx]]\n\nplt.figure(figsize=(6, 5))\nsc = plt.scatter(X2[:, 0], X2[:, 1], c=y_pos, s=10, cmap='coolwarm', alpha=0.6)\nplt.xlabel(f'Response to {best_name}')\nplt.ylabel(f'Response to {second_name}')\nplt.title('Two-odorant feature space (color = y position)')\nplt.colorbar(sc, label='y position')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "fit-two-explain",
   "metadata": {},
   "source": [
    "### 2.2 Fit and evaluate\n",
    "\n",
    "The scikit-learn API is exactly the same — the only change is that\n",
    "`X` now has two columns instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-two",
   "metadata": {},
   "outputs": [],
   "source": "X2_train, X2_test, y2_train, y2_test = train_test_split(\n    X2, y_pos, test_size=0.2, random_state=42\n)\n\nmodel2 = LinearRegression()\nmodel2.fit(X2_train, y2_train)\n\ny2_pred = model2.predict(X2_test)\nr2_two = r2_score(y2_test, y2_pred)\nrmse_two = np.sqrt(mean_squared_error(y2_test, y2_pred))\n\nprint(f\"Coefficients: {model2.coef_}\")\nprint(f\"Intercept:    {model2.intercept_:.4f}\")\nprint(f\"R² (test):    {r2_two:.4f}\")\nprint(f\"RMSE (test):  {rmse_two:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-two",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y2_test, y2_pred, s=10, alpha=0.4)\n",
    "lims = [min(y2_test.min(), y2_pred.min()), max(y2_test.max(), y2_pred.max())]\n",
    "plt.plot(lims, lims, 'k--', linewidth=1, label='Perfect prediction')\n",
    "plt.xlabel('Actual y position')\n",
    "plt.ylabel('Predicted y position')\n",
    "plt.title(f'Two-feature regression (R²={r2_two:.3f})')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incremental",
   "metadata": {},
   "source": "### 2.3 How does performance grow with more features?\n\nLet's add odorants one at a time (in order of their single-feature\nranking) and see how R² improves.  Here we use all glomeruli so the\ncomparison is fair across feature counts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incremental-features",
   "metadata": {},
   "outputs": [],
   "source": "ranked_indices = [list(odorant_names).index(n) for n in r2_all_sorted.index]\n\nr2_curve = []\nfor k in range(1, len(ranked_indices) + 1):\n    cols = ranked_indices[:k]\n    Xk = X[:, cols]\n    Xk_tr, Xk_te, yk_tr, yk_te = train_test_split(\n        Xk, y_pos, test_size=0.2, random_state=42\n    )\n    mk = LinearRegression().fit(Xk_tr, yk_tr)\n    r2_curve.append(r2_score(yk_te, mk.predict(Xk_te)))\n\nplt.figure(figsize=(7, 4))\nplt.plot(range(1, len(r2_curve) + 1), r2_curve, '.-')\nplt.xlabel('Number of features (odorants, ranked by single-feature R²)')\nplt.ylabel('R² (test set)')\nplt.title('Diminishing returns: R² vs. number of features')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "stage2-reflection",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Adding a second odorant can improve the fit, but each additional\n",
    "feature contributes less and less.  Notice also that with many\n",
    "features, performance can *decrease* on the test set — a sign of\n",
    "**overfitting**.  When the model has too many free parameters relative\n",
    "to the number of samples, it starts fitting noise rather than signal.\n",
    "\n",
    "This motivates two ideas we will explore next:\n",
    "- Using **all** features at once, but with **regularization** to\n",
    "  combat overfitting.\n",
    "- Predicting **both** x and y simultaneously (**multi-target regression**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage3-header",
   "metadata": {},
   "source": "---\n## 3. Multi-target, multi-feature regression\n\nNow we use **all 59 odorants** as features and predict **both x and y\ncoordinates** simultaneously.  The model learns a mapping:\n\n$$[\\hat{x}, \\hat{y}] = \\mathbf{X} \\mathbf{W} + \\mathbf{b}$$\n\nwhere **W** is a (59 × 2) weight matrix and **b** is a (2,) bias\nvector.\n\nWith the full odorant panel as features, we no longer need per-odorant\nfiltering — every glomerulus has a meaningful 59-dimensional response\nprofile even if many individual entries are zero.  The pattern of\n*which* odorants a glomerulus does and does not respond to is itself\ninformative."
  },
  {
   "cell_type": "markdown",
   "id": "multi-setup",
   "metadata": {},
   "source": [
    "### 3.1 Prepare multi-target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.column_stack([x_pos, y_pos])  # shape: (n_glomeruli, 2)\n",
    "print(f\"Features X:  {X.shape}\")\n",
    "print(f\"Targets  Y:  {Y.shape}  (columns: x, y)\")\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ols-multi",
   "metadata": {},
   "source": [
    "### 3.2 Ordinary least-squares (baseline)\n",
    "\n",
    "Scikit-learn's `LinearRegression` natively supports multiple targets —\n",
    "just pass a 2-D `y` array.  It fits an independent regression for\n",
    "each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-ols-multi",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_ols = model_ols.predict(X_test)\n",
    "\n",
    "r2_x_ols = r2_score(Y_test[:, 0], Y_pred_ols[:, 0])\n",
    "r2_y_ols = r2_score(Y_test[:, 1], Y_pred_ols[:, 1])\n",
    "\n",
    "print(f\"OLS  — R² for x: {r2_x_ols:.4f},  R² for y: {r2_y_ols:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overfit-explain",
   "metadata": {},
   "source": [
    "With 59 features and a modest number of samples, ordinary\n",
    "least-squares may be overfitting.  **Regularization** penalizes large\n",
    "coefficients to keep the model simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ridge-explain",
   "metadata": {},
   "source": [
    "### 3.3 Ridge regression (L2 regularization)\n",
    "\n",
    "Ridge regression adds a penalty proportional to the *sum of squared\n",
    "coefficients*:\n",
    "\n",
    "$$\\text{Loss} = \\sum_i (y_i - \\hat{y}_i)^2 + \\alpha \\sum_j w_j^2$$\n",
    "\n",
    "The hyperparameter $\\alpha$ controls the strength of the penalty.\n",
    "Larger $\\alpha$ → stronger shrinkage → simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model_ridge = Ridge(alpha=1.0)\n",
    "model_ridge.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_ridge = model_ridge.predict(X_test)\n",
    "\n",
    "r2_x_ridge = r2_score(Y_test[:, 0], Y_pred_ridge[:, 0])\n",
    "r2_y_ridge = r2_score(Y_test[:, 1], Y_pred_ridge[:, 1])\n",
    "\n",
    "print(f\"Ridge — R² for x: {r2_x_ridge:.4f},  R² for y: {r2_y_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cv-explain",
   "metadata": {},
   "source": [
    "### 3.4 Choosing alpha with cross-validation\n",
    "\n",
    "How do we pick the best $\\alpha$?  **Cross-validation** systematically\n",
    "evaluates different values by repeatedly splitting the *training* set\n",
    "into folds:\n",
    "\n",
    "1. Hold out one fold as a validation set.\n",
    "2. Train on the remaining folds.\n",
    "3. Score on the held-out fold.\n",
    "4. Average scores across all folds.\n",
    "\n",
    "`RidgeCV` does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fit-ridgecv",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "alphas = np.logspace(-2, 4, 50)\n",
    "\n",
    "# RidgeCV with multi-target: we fit on y-position column first to pick alpha,\n",
    "# then apply that alpha to the full multi-target problem.\n",
    "ridge_cv = RidgeCV(alphas=alphas, cv=5)\n",
    "ridge_cv.fit(X_train, Y_train[:, 1])  # tune on y target\n",
    "\n",
    "best_alpha = ridge_cv.alpha_\n",
    "print(f\"Best alpha (via 5-fold CV): {best_alpha:.4f}\")\n",
    "\n",
    "# Refit with the chosen alpha on both targets\n",
    "model_best = Ridge(alpha=best_alpha)\n",
    "model_best.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_best = model_best.predict(X_test)\n",
    "\n",
    "r2_x_best = r2_score(Y_test[:, 0], Y_pred_best[:, 0])\n",
    "r2_y_best = r2_score(Y_test[:, 1], Y_pred_best[:, 1])\n",
    "\n",
    "print(f\"Best  — R² for x: {r2_x_best:.4f},  R² for y: {r2_y_best:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": "### 3.5 Compare all models\n\nAll rows below use the same sample (all glomeruli, no filtering)\nso the R² values are directly comparable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-table",
   "metadata": {},
   "outputs": [],
   "source": "results = pd.DataFrame({\n    'Model': [\n        f'Single odorant — {best_name}',\n        f'Two odorants — {best_name} + {second_name}',\n        'All 59 odorants — OLS',\n        'All 59 odorants — Ridge (α=1)',\n        f'All 59 odorants — Ridge (α={best_alpha:.2f})',\n    ],\n    'R² (y)': [\n        r2_all_glom[best_idx],\n        r2_two,\n        r2_y_ols,\n        r2_y_ridge,\n        r2_y_best,\n    ],\n    'R² (x)': [\n        np.nan,\n        np.nan,\n        r2_x_ols,\n        r2_x_ridge,\n        r2_x_best,\n    ],\n})\nresults"
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "### 3.6 Visualize predictions\n",
    "\n",
    "Let's plot the predicted vs. actual positions for our best model on\n",
    "the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n",
    "\n",
    "# Predicted vs actual — x\n",
    "axes[0].scatter(Y_test[:, 0], Y_pred_best[:, 0], s=10, alpha=0.4)\n",
    "lims = [Y_test[:, 0].min(), Y_test[:, 0].max()]\n",
    "axes[0].plot(lims, lims, 'k--', linewidth=1)\n",
    "axes[0].set(xlabel='Actual x', ylabel='Predicted x',\n",
    "            title=f'x position (R²={r2_x_best:.3f})')\n",
    "\n",
    "# Predicted vs actual — y\n",
    "axes[1].scatter(Y_test[:, 1], Y_pred_best[:, 1], s=10, alpha=0.4)\n",
    "lims = [Y_test[:, 1].min(), Y_test[:, 1].max()]\n",
    "axes[1].plot(lims, lims, 'k--', linewidth=1)\n",
    "axes[1].set(xlabel='Actual y', ylabel='Predicted y',\n",
    "            title=f'y position (R²={r2_y_best:.3f})')\n",
    "\n",
    "# Spatial map: actual vs predicted\n",
    "axes[2].scatter(Y_test[:, 0], Y_test[:, 1], s=12, alpha=0.4, label='Actual')\n",
    "axes[2].scatter(Y_pred_best[:, 0], Y_pred_best[:, 1], s=12, alpha=0.4,\n",
    "                marker='x', label='Predicted')\n",
    "# Draw lines connecting actual → predicted\n",
    "for i in range(len(Y_test)):\n",
    "    axes[2].plot([Y_test[i, 0], Y_pred_best[i, 0]],\n",
    "                 [Y_test[i, 1], Y_pred_best[i, 1]],\n",
    "                 'gray', linewidth=0.3, alpha=0.3)\n",
    "axes[2].set(xlabel='x position', ylabel='y position',\n",
    "            title='Actual vs. predicted locations')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coef-header",
   "metadata": {},
   "source": [
    "### 3.7 Which odorants matter most?\n",
    "\n",
    "The Ridge coefficients tell us how much each odorant contributes to\n",
    "predicting each target.  Larger absolute coefficients indicate\n",
    "stronger influence (though correlated features share weight, so\n",
    "interpret with care)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coef-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(\n",
    "    model_best.coef_.T,\n",
    "    index=odorant_names,\n",
    "    columns=['x coef', 'y coef'],\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, col in zip(axes, ['x coef', 'y coef']):\n",
    "    sorted_coefs = coef_df[col].sort_values()\n",
    "    colors = ['steelblue' if v >= 0 else 'salmon' for v in sorted_coefs]\n",
    "    ax.barh(range(len(sorted_coefs)), sorted_coefs.values, color=colors)\n",
    "    ax.set_yticks(range(len(sorted_coefs)))\n",
    "    ax.set_yticklabels(sorted_coefs.index, fontsize=6)\n",
    "    ax.set_xlabel('Coefficient')\n",
    "    ax.set_title(f'Ridge coefficients — {col}')\n",
    "    ax.axvline(0, color='gray', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Lasso regression:** Replace `Ridge` with `Lasso` (L1 penalty).\n",
    "   How many coefficients are driven to exactly zero?  What does this\n",
    "   tell you about feature selection?\n",
    "\n",
    "2. **High concentration:** Re-run the analysis using the\n",
    "   high-concentration data (`high` instead of `low`).  Does\n",
    "   prediction improve?\n",
    "\n",
    "3. **Non-linear models:** Try `sklearn.ensemble.RandomForestRegressor`\n",
    "   or `sklearn.neighbors.KNeighborsRegressor` in place of\n",
    "   `LinearRegression`.  Do non-linear models capture additional\n",
    "   structure?\n",
    "\n",
    "4. **Subject effects:** The data pools glomeruli from four animals.\n",
    "   Train on three subjects and test on the fourth.  Does the spatial\n",
    "   code generalize across animals?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | What we learned |\n",
    "|---------|----------------|\n",
    "| Simple linear regression | One feature can capture limited spatial signal |\n",
    "| Multiple regression | Adding features improves predictions, with diminishing returns |\n",
    "| Train/test split | Essential to detect overfitting |\n",
    "| Multi-output regression | Predict both coordinates simultaneously |\n",
    "| Regularization (Ridge) | Penalty on coefficients prevents overfitting with many features |\n",
    "| Cross-validation | Principled way to choose hyperparameters |\n",
    "| Feature importance | Ridge coefficients reveal which odorants drive spatial predictions |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}